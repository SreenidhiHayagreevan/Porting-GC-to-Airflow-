# -*- coding: utf-8 -*-
"""Airflow_ETL

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DDd9HFzdoyMfGE-dWmCz1W5kqk0Xld7N
"""

from airflow import DAG
from airflow.models import Variable
from airflow.decorators import task
from datetime import datetime
import snowflake.connector
import requests

def return_snowflake_conn():
    # Snowflake connection information retrieved via Airflow connection object
    conn = snowflake.connector.connect(
        user=Variable.get('snowflake_username'),
        password=Variable.get('snowflake_password'),
        account=Variable.get('snowflake_account'),
        warehouse='compute_wh',
        database='dev'
    )
    return conn.cursor()

@task
def extract():
    # Retrieve API key and URL template from Airflow Variables
    api_key = Variable.get('vantage_api_key')
    url_template = Variable.get("Alpha_url")

    # Define the symbol
    symbol = 'ISRG'

    # Format the URL with the desired symbol and API key
    url = url_template.format(symbol=symbol, vantage_api_key=api_key)

    # Make the API request
    response = requests.get(url)
    data = response.json()

    return data


@task
def transform(data):
    transformed_results = []
    # Transform the data if it has the expected structure
    if 'Time Series (Daily)' in data:
        for date, values in data['Time Series (Daily)'].items():
            transformed = {
                'date': date,
                'open': values['1. open'],
                'high': values['2. high'],
                'low': values['3. low'],
                'close': values['4. close'],
                'volume': values['5. volume']
            }
            transformed_results.append(transformed)
    return transformed_results

@task
def load(transformed_data):
    # Use the same symbol as in the extract function
    symbol = 'ISRG'
    target_table = "DEV.RAW_DATA.ETL_Stock_Price"

    if not transformed_data:
        print("No data to load.")
        return

    try:
        # Establish Snowflake connection within the load task
        cur = return_snowflake_conn()

        # Begin a Snowflake transaction
        cur.execute("BEGIN;")

        # Create or replace the target table in Snowflake
        cur.execute(f"""
        CREATE OR REPLACE TABLE {target_table} (
          date DATE,
          open NUMBER,
          high NUMBER,
          low NUMBER,
          close NUMBER,
          volume NUMBER,
          symbol VARCHAR
        )
        """)

        # SQL statement to insert data into the table
        insert_sql = f"""
        INSERT INTO {target_table} (date, open, high, low, close, volume, symbol)
        VALUES (%s, %s, %s, %s, %s, %s, %s)
        """

        # Insert each record from the transformed data into Snowflake
        for record in transformed_data:
            cur.execute(insert_sql, (
                record['date'],
                record['open'],
                record['high'],
                record['low'],
                record['close'],
                record['volume'],
                symbol
            ))

        # Commit the transaction
        cur.execute("COMMIT;")
        print("Data loaded successfully.")

    except Exception as e:
        # If an error occurs, rollback the transaction
        cur.execute("ROLLBACK;")
        print("An error occurred while loading data:", str(e))
        raise e
    finally:
        cur.close()  # Close the cursor when done

# Define the DAG
with DAG(
    dag_id='ETL_Airflow',
    start_date=datetime(2024, 10, 1),
    catchup=False,
    tags=['ETL'],
    schedule='30 4 * * *'  # Run at 2:00 AM every day
) as dag:

    # Define the task execution order: extract -> transform -> load
    extracted_data = extract()
    transformed_data = transform(extracted_data)
    load(transformed_data)

